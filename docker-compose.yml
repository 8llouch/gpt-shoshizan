services:
  postgres:
    image: postgres:15-alpine
    container_name: gpt-postgres
    env_file:
      - ./.env
      - ./.env.secrets
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
    ports:
      - "8989:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: apache/kafka:4.0.1-rc2
    container_name: gpt-kafka
    # Un seul broker, KRaft, listeners internes+externes (ports distincts)
    env_file:
      - ./.env
    environment:
      KAFKA_PROCESS_ROLES: ${KAFKA_PROCESS_ROLES}
      KAFKA_NODE_ID: ${KAFKA_NODE_ID}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
      KAFKA_LISTENERS: ${KAFKA_LISTENERS}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_CONTROLLER_LISTENER_NAMES: ${KAFKA_CONTROLLER_LISTENER_NAMES}
      KAFKA_CONTROLLER_QUORUM_VOTERS: ${KAFKA_CONTROLLER_QUORUM_VOTERS}
      KAFKA_INTER_BROKER_LISTENER_NAME: ${KAFKA_INTER_BROKER_LISTENER_NAME}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: ${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: ${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: ${KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
    ports:
      - "9092:29092" # host:container (EXTERNAL listener)
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "kafka-topics --bootstrap-server localhost:29092 --list >/dev/null 2>&1 || exit 1",
        ]
      start_period: 20s
      interval: 10s
      timeout: 5s
      retries: 5

  ollama:
    image: ollama/ollama:latest
    container_name: gpt-ollama
    ports:
      - "11434:11434"
    env_file:
      - ./.env
    environment:
      OLLAMA_HOST: ${OLLAMA_HOST_BIND}
      OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS}
      OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE}
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 10G
    healthcheck:
      test: ["CMD", "/bin/ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 20s

  ollama-init:
    image: ollama/ollama:latest
    container_name: gpt-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    env_file:
      - ./.env
    environment:
      OLLAMA_HOST: ${OLLAMA_INTERNAL_URL}
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Starting to pull and preload models from OLLAMA_MODELS: $${OLLAMA_MODELS}"
        OLD_IFS="$$IFS"
        IFS=','
        for model in $${OLLAMA_MODELS}; do
          echo "Pulling model: $$model"
          ollama pull $$model || echo "Failed to pull $$model, continuing..."
        done
        IFS="$$OLD_IFS"
        echo "All models pulled successfully"
        
        echo "Preloading models into memory..."
        IFS=','
        for model in $${OLLAMA_MODELS}; do
          echo "Preloading model into memory: $$model"
          ollama run $$model "Hello" --verbose || echo "Failed to preload $$model, continuing..."
          echo "Model $$model preloaded"
        done
        IFS="$$OLD_IFS"
        echo "All models preloaded into memory"
    restart: "no"

  kafka-init:
    image: apache/kafka:4.0.1-rc2
    container_name: gpt-kafka-init
    depends_on:
      kafka:
        condition: service_started
    env_file:
      - ./.env
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for Kafka to be ready..."
        sleep 10
        echo "Creating topics..."
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic input.created --partitions 1 --replication-factor 1
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic output.created --partitions 1 --replication-factor 1
        echo "Topics created successfully"
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list
    restart: "no"

  gpt-gateway:
    build:
      context: .
      dockerfile: Dockerfile.gateway
    container_name: gpt-gateway
    env_file:
      - ./.env
      - ./.env.secrets
    environment:
      GATEWAY_PORT: ${GATEWAY_PORT}
      FRONT_END_URL: ${FRONT_END_URL}
      API_SERVICE_URL: ${API_SERVICE_URL}
      KAFKA_PRODUCER_URL: ${KAFKA_PRODUCER_URL}
      KAFKA_CONSUMER_URL: ${KAFKA_CONSUMER_URL}
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      DB_USERNAME: ${DB_USERNAME}
      DB_DATABASE: ${DB_DATABASE}
      DB_SYNCHRONIZE: ${DB_SYNCHRONIZE}
      DB_LOGGING: ${DB_LOGGING}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
      # OpenTelemetry
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT}
      OTEL_SERVICE_NAME_GATEWAY: ${OTEL_SERVICE_NAME_GATEWAY}
      ENABLE_TRACING: ${ENABLE_TRACING}
      OTEL_LOG_LEVEL: ${OTEL_LOG_LEVEL}
    ports:
      - "3000:3000"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_started
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully

  gpt-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: gpt-api
    env_file:
      - ./.env
      - ./.env.secrets
    environment:
      API_PORT: ${API_PORT}
      API_HOST: ${API_HOST}
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      DB_USERNAME: ${DB_USERNAME}
      DB_DATABASE: ${DB_DATABASE}
      DB_SYNCHRONIZE: ${DB_SYNCHRONIZE}
      DB_LOGGING: ${DB_LOGGING}
      NODE_ENV: ${NODE_ENV}
      # OpenTelemetry
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT}
      OTEL_SERVICE_NAME_API: ${OTEL_SERVICE_NAME_API}
      ENABLE_TRACING: ${ENABLE_TRACING}
      OTEL_LOG_LEVEL: ${OTEL_LOG_LEVEL}
    ports:
      - "3001:3001"
    depends_on:
      postgres:
        condition: service_healthy

  gpt-producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: gpt-producer
    env_file:
      - ./.env
      - ./.env.secrets
    environment:
      - PRODUCER_PORT=${PRODUCER_PORT}
      - FRONT_END_URL=${FRONT_END_URL}
      # Compat actuel 
      - KAFKA_BROKER=${KAFKA_BROKER}
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      # OpenTelemetry 
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT}
      - OTEL_SERVICE_NAME_PRODUCER=${OTEL_SERVICE_NAME_PRODUCER}
      - ENABLE_TRACING=${ENABLE_TRACING}
      - OTEL_LOG_LEVEL=${OTEL_LOG_LEVEL}
    depends_on:
      kafka:
        condition: service_started
      kafka-init:
        condition: service_completed_successfully

  gpt-consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    container_name: gpt-consumer
    env_file:
      - ./.env
      - ./.env.secrets
    environment:
      CONSUMER_PORT: ${CONSUMER_PORT}
      KAFKA_CONSUMER_GROUP_ID: ${KAFKA_CONSUMER_GROUP_ID}
      KAFKA_BROKER: ${KAFKA_BROKER}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      DB_USERNAME: ${DB_USERNAME}
      DB_DATABASE: ${DB_DATABASE}
      DB_SYNCHRONIZE: ${DB_SYNCHRONIZE}
      # OpenTelemetry 
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT}
      OTEL_SERVICE_NAME_CONSUMER: ${OTEL_SERVICE_NAME_CONSUMER}
      ENABLE_TRACING: ${ENABLE_TRACING}
      OTEL_LOG_LEVEL: ${OTEL_LOG_LEVEL}
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_started
      kafka-init:
        condition: service_completed_successfully

  gpt-ui:
    build:
      context: .
      dockerfile: Dockerfile.ui
      args:
        VITE_API_BASE_URL: ${VITE_API_BASE_URL}
        # VITE_DEV_BEARER_TOKEN removed - should not be baked into image
    container_name: gpt-ui
    ports:
      - "5173:80"
    depends_on:
      gpt-gateway:
        condition: service_started

  
  # OpenTelemetry Collector - Central telemetry pipeline
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: gpt-otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./observability/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # Health check
      - "55679:55679" # zpages extension
    env_file:
      - ./.env
    environment:
      - OTEL_RESOURCE_ATTRIBUTES=service.name=otel-collector,${OTEL_RESOURCE_ATTRIBUTES}
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - default

  # Jaeger - Distributed tracing backend
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: gpt-jaeger
    env_file:
      - ./.env
    ports:
      - "16686:16686" # Jaeger UI
      - "14250:14250" # gRPC collector
      - "6831:6831/udp" # Jaeger agent (UDP)
    environment:
      - COLLECTOR_OTLP_ENABLED=${JAEGER_COLLECTOR_OTLP_ENABLED}
      - SPAN_STORAGE_TYPE=${JAEGER_SPAN_STORAGE_TYPE}
      - MEMORY_MAX_TRACES=${JAEGER_MEMORY_MAX_TRACES}
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:16686/"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - default

  # Prometheus - Metrics collection and storage
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: gpt-prometheus
    env_file:
      - ./.env
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME}'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./observability/prometheus-rules.yml:/etc/prometheus/rules/rules.yml
      - prometheus_data:/prometheus
    ports:
      - "${PROMETHEUS_PORT}:9090" # Prometheus UI
    depends_on:
      - otel-collector
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - default

  # Grafana - Metrics and traces visualization
  grafana:
    image: grafana/grafana-oss:10.2.0
    container_name: gpt-grafana
    env_file:
      - ./.env
    ports:
      - "${GRAFANA_PORT}:3000" # Grafana UI (changed to avoid conflict with Gateway)
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=${GRAFANA_USERS_ALLOW_SIGN_UP}
      - GF_AUTH_ANONYMOUS_ENABLED=${GRAFANA_AUTH_ANONYMOUS_ENABLED}
      - GF_AUTH_ANONYMOUS_ORG_ROLE=${GRAFANA_AUTH_ANONYMOUS_ORG_ROLE}
      - GF_INSTALL_PLUGINS=${GRAFANA_INSTALL_PLUGINS}
    volumes:
      - ./observability/grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
      - ./observability/grafana-dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
      - jaeger
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - default

volumes:
  postgres_data:
  kafka_data:
  ollama_data:
  prometheus_data:
  grafana_data:
